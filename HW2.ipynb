{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='black'>Q1. (Multinomail Classification)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Randomly shuffle each dataset and use 80% of data for training and 20% for testing. And repeat 10 times and report the average accuracy for each class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filename1 = \"C:\\\\Users\\\\vwslz\\\\Desktop\\\\CSE512\\\\HW2\\\\data1.txt\"\n",
    "filename2 = \"C:\\\\Users\\\\vwslz\\\\Desktop\\\\CSE512\\\\HW2\\\\data2.txt\"\n",
    "filename3 = \"C:\\\\Users\\\\vwslz\\\\Desktop\\\\CSE512\\\\HW2\\\\data3.txt\"\n",
    "data1 = pd.read_csv(filename1, delimiter='\\t', header=None, names = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'label'])\n",
    "data2 = pd.read_csv(filename2, delimiter='\\t', header=None, names = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'label'])\n",
    "data3 = pd.read_csv(filename3, delimiter='\\t', header=None, names = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'label'])\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "df3 = pd.DataFrame(data3)\n",
    "\n",
    "data = pd.concat([data1, data2, data3], ignore_index=True)\n",
    "\n",
    "data_X = data.iloc[:, 0:5]\n",
    "data_Y = data['label']\n",
    "\n",
    "#one hot key\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(data_Y.values.reshape(-1,1))\n",
    "\n",
    "randomShuffle = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n",
    "\n",
    "max_accuracy = 0\n",
    "res_W = 0\n",
    "res_b = 0\n",
    "res_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in randomShuffle.split(data):\n",
    "    X_train = data_X.iloc[train_index, :].values\n",
    "    Y_train = onehot_encoded[train_index, :]\n",
    "\n",
    "    X_test = data_X.iloc[test_index, :].values\n",
    "    Y_test = onehot_encoded[test_index, :]\n",
    "\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 5])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "    W = tf.Variable(tf.random_uniform([5, 3])*0.5, name='weight')\n",
    "    b = tf.Variable(tf.random_uniform([1, 3]), name='bias')\n",
    "\n",
    "    logits = tf.matmul(X, W) + b\n",
    "    softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), 0)\n",
    "\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(softmax), axis = 1))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(cost)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(9000):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={\n",
    "            X: X_train,\n",
    "            Y: Y_train})\n",
    "        #if (step > 8990):\n",
    "        #    print(cost_val)\n",
    "\n",
    "    W_val, b_val, _ = sess.run([W, b, train], feed_dict={\n",
    "            X: X_train,\n",
    "            Y: Y_train})\n",
    "            \n",
    "    correct_prediction = tf.equal(tf.argmax(softmax,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy_val = sess.run(accuracy, feed_dict={X: X_test, Y: Y_test})\n",
    "    res_accuracy.append(accuracy_val)\n",
    "    if accuracy_val > max_accuracy:\n",
    "        res_W = W_val\n",
    "        res_b = b_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71666664, 0.76666665, 0.75, 0.8, 0.78333336, 0.68333334, 0.8666667, 0.6666667, 0.8, 0.7]\n"
     ]
    }
   ],
   "source": [
    "print(res_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Report the best w's and b's from your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03942344  0.09809706 -0.20068134]\n",
      " [-0.06751192  0.04120848  0.04659131]\n",
      " [-1.2031257   0.5291206  -0.4987563 ]\n",
      " [-0.36747205 -0.19375043 -0.0491154 ]\n",
      " [-0.4132177   0.1624189   0.20662223]]\n",
      "[[0.0805372  0.23769689 0.20414364]]\n"
     ]
    }
   ],
   "source": [
    "print(res_W)\n",
    "print(res_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='black'>Q2. (Multinomail Classification & Feature Reduction)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "featurenames = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n",
    "data_shuffle = shuffle(data)\n",
    "accuracy_avg_max = 0\n",
    "n_fold = 5\n",
    "\n",
    "kf = KFold(n_splits = n_fold, shuffle = False, random_state = 2)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "W = tf.Variable(tf.random_uniform([4, 3]), name='weight')\n",
    "b = tf.Variable(tf.random_uniform([1, 3]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of model to drop feature 1: 0.7133333444595337\n",
      "The accuracy of model to drop feature 2: 0.6933333277702332\n",
      "The accuracy of model to drop feature 3: 0.526666671037674\n",
      "The accuracy of model to drop feature 4: 0.65\n",
      "The accuracy of model to drop feature 5: 0.7666666626930236\n"
     ]
    }
   ],
   "source": [
    "for index in range(5):\n",
    "    accuracy_sum = 0\n",
    "    for train_index, test_index in kf.split(data_shuffle):\n",
    "        train = data_shuffle.iloc[train_index]\n",
    "        test =  data_shuffle.iloc[test_index]\n",
    "\n",
    "        data_X_train = train.iloc[:, 0:5].drop(columns=[featurenames[index]]).values\n",
    "        data_Y_train = onehot_encoder.fit_transform(train['label'].values.reshape(-1,1))\n",
    "        data_X_test = test.iloc[:, 0:5].drop(columns=[featurenames[index]]).values\n",
    "        data_Y_test = onehot_encoder.fit_transform(test['label'].values.reshape(-1,1))\n",
    "\n",
    "        logits = tf.matmul(X, W) + b\n",
    "        softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), 0)\n",
    "\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(softmax), axis = 1))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train = optimizer.minimize(cost)\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for step in range(9000):\n",
    "            cost_val, _ = sess.run([cost, train], feed_dict={\n",
    "                X: data_X_train,\n",
    "                Y: data_Y_train})\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(softmax,1), tf.argmax(Y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        accuracy_val = sess.run(accuracy, feed_dict={X: data_X_test, Y: data_Y_test})\n",
    "        accuracy_sum += accuracy_val\n",
    "\n",
    "    accuracy_avg = accuracy_sum / n_fold\n",
    "    print(\"The accuracy of model to drop feature \" + str(index + 1) + \": \" + str(accuracy_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above accuracies, the mode dropping feature 5 has the highest accuracy. In other words, feature 5 has the least influence on the results. So I would like to select the other features for my classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "W = tf.Variable(tf.random_uniform([4, 3])*0.5, name='weight')\n",
    "b = tf.Variable(tf.random_uniform([1, 3]), name='bias')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "max_accuracy = 0\n",
    "res_W = 0\n",
    "res_b = 0\n",
    "res_accuracy = []\n",
    "\n",
    "data_X = data.iloc[:, 0:5].drop(columns=['feature5'])\n",
    "data_Y = data['label']\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in randomShuffle.split(data):  \n",
    "    X_train = data_X.iloc[train_index, :].values\n",
    "    Y_train = onehot_encoded[train_index, :]\n",
    "\n",
    "    X_test = data_X.iloc[test_index, :].values\n",
    "    Y_test = onehot_encoded[test_index, :]\n",
    "    \n",
    "    logits = tf.matmul(X, W) + b\n",
    "    softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), 0)\n",
    "\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(softmax), axis = 1))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(cost)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(9000):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={\n",
    "            X: X_train,\n",
    "            Y: Y_train})\n",
    "\n",
    "    W_val, b_val, _ = sess.run([W, b, train], feed_dict={\n",
    "            X: X_train,\n",
    "            Y: Y_train})\n",
    "            \n",
    "    correct_prediction = tf.equal(tf.argmax(softmax,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy_val = sess.run(accuracy, feed_dict={X: X_test, Y: Y_test})\n",
    "    res_accuracy.append(accuracy_val)\n",
    "    if accuracy_val > max_accuracy:\n",
    "        res_W = W_val\n",
    "        res_b = b_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Randomly shuffle each dataset and use 80% of data for training and 20% for testing. And repeat 10 times and report the average accuracy for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71666664, 0.76666665, 0.8, 0.81666666, 0.78333336, 0.7, 0.8666667, 0.78333336, 0.78333336, 0.73333335]\n"
     ]
    }
   ],
   "source": [
    "print (res_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Report the best w's and b's from your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09435022  0.11295945 -0.16324764]\n",
      " [-0.04381175  0.02561875  0.0136826 ]\n",
      " [-0.37269166  0.5630736  -0.1602366 ]\n",
      " [-0.72112477 -0.04416402  0.12874234]]\n",
      "[[0.01960063 0.07007885 0.9649017 ]]\n"
     ]
    }
   ],
   "source": [
    "print(res_W)\n",
    "print(res_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Compare the accuracy with Q1.(a). Was it better or worse by how much? Explain your reason for selecting 4 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the best accuracy is the same, the average accuracy is improved. In Q1(a), the average accuracy is 0.753333339 while in Q2(a) the average accuracy is 0.775000008. Decrease the number features can help the model decrease the influence from noises. In other words, it helps to reduce variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='black'>Q3. (SVM)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import seaborn as sns; sns.set(font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = \"C:\\\\Users\\\\vwslz\\\\Desktop\\\\CSE512\\\\HW2\\\\data1.txt\"\n",
    "filename2 = \"C:\\\\Users\\\\vwslz\\\\Desktop\\\\CSE512\\\\HW2\\\\data2.txt\"\n",
    "filename3 = \"C:\\\\Users\\\\vwslz\\\\Desktop\\\\CSE512\\\\HW2\\\\data3.txt\"\n",
    "data1 = pd.read_csv(filename1, delimiter='\\t', header=None, names = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'label'])\n",
    "data2 = pd.read_csv(filename2, delimiter='\\t', header=None, names = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'label'])\n",
    "data3 = pd.read_csv(filename3, delimiter='\\t', header=None, names = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'label'])\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "df3 = pd.DataFrame(data3)\n",
    "\n",
    "data = pd.concat([data1, data2, data3], ignore_index=True)\n",
    "data_shuffle = shuffle(data)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "data_X = data.iloc[:, 0:5].values\n",
    "#data_Y = onehot_encoder.fit_transform(data['label'].values.reshape(-1,1))\n",
    "data_Y = data['label'].values\n",
    "\n",
    "X_train_shuffle, X_test_shuffle, Y_train_shuffle, Y_test_shuffle = train_test_split(\n",
    "    data_X, data_Y, test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters set found on development set:\n",
      "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "                     'C': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]}]\n",
    "\n",
    "score='accuracy'\n",
    "\n",
    "print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "print()\n",
    "\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring=score)\n",
    "clf.fit(X_train_shuffle, Y_train_shuffle)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)\n",
    "#print()\n",
    "#print(\"Grid scores on development set:\")\n",
    "#means = clf.cv_results_['mean_test_score']\n",
    "#stds = clf.cv_results_['std_test_score']\n",
    "#for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "#    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#          % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "res_accuracy = []\n",
    "\n",
    "model = svm.SVC(kernel='rbf', C=100, gamma=0.01)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    data_X, data_Y, test_size=0.2)\n",
    "    \n",
    "    accuracy = accuracy_score(Y_test, model.predict(X_test))\n",
    "    res_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8666666666666667, 0.8666666666666667, 0.9166666666666666, 0.8833333333333333, 0.9, 0.9, 0.8166666666666667, 0.85, 0.9333333333333333, 0.8333333333333334]\n"
     ]
    }
   ],
   "source": [
    "print(res_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
